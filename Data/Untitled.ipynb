{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a76a14-9f5e-416b-9444-37ac75fb3691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AGGREGATE RESULTS: Files 001-150\n",
      "============================================================\n",
      "Current Dir:  /Users/kaylali/Documents/Documents/Stanford/MSE311/Project/University_Course_Scheduler/Data\n",
      "Project Root: /Users/kaylali/Documents/Documents/Stanford/MSE311/Project/University_Course_Scheduler\n",
      "Tracking Dir: /Users/kaylali/Documents/Documents/Stanford/MSE311/Project/University_Course_Scheduler/Results/tracking_logs\n",
      "Plots Dir:    /Users/kaylali/Documents/Documents/Stanford/MSE311/Project/University_Course_Scheduler/Results/plots\n",
      "\n",
      "ðŸ“‚ Loading tracking logs...\n",
      "  âœ“ Loaded: schedule_input_001_log.json\n",
      "  âœ“ Loaded: schedule_input_002_log.json\n",
      "  âœ“ Loaded: schedule_input_003_log.json\n",
      "  âœ“ Loaded: schedule_input_004_log.json\n",
      "  âœ“ Loaded: schedule_input_005_log.json\n",
      "  âœ“ Loaded: schedule_input_006_log.json\n",
      "  âœ“ Loaded: schedule_input_007_log.json\n",
      "  âœ“ Loaded: schedule_input_008_log.json\n",
      "  âœ“ Loaded: schedule_input_009_log.json\n",
      "  âœ“ Loaded: schedule_input_010_log.json\n",
      "  âœ“ Loaded: schedule_input_011_log.json\n",
      "  âœ“ Loaded: schedule_input_012_log.json\n",
      "  âœ“ Loaded: schedule_input_013_log.json\n",
      "  âœ“ Loaded: schedule_input_014_log.json\n",
      "  âœ“ Loaded: schedule_input_015_log.json\n",
      "  âœ“ Loaded: schedule_input_016_log.json\n",
      "  âœ“ Loaded: schedule_input_017_log.json\n",
      "  âœ“ Loaded: schedule_input_018_log.json\n",
      "  âœ“ Loaded: schedule_input_019_log.json\n",
      "  âœ“ Loaded: schedule_input_020_log.json\n",
      "  âœ“ Loaded: schedule_input_021_log.json\n",
      "  âœ“ Loaded: schedule_input_022_log.json\n",
      "  âœ“ Loaded: schedule_input_023_log.json\n",
      "  âœ“ Loaded: schedule_input_024_log.json\n",
      "  âœ“ Loaded: schedule_input_025_log.json\n",
      "  âœ“ Loaded: schedule_input_026_log.json\n",
      "  âœ“ Loaded: schedule_input_027_log.json\n",
      "  âœ“ Loaded: schedule_input_028_log.json\n",
      "  âœ“ Loaded: schedule_input_029_log.json\n",
      "  âœ“ Loaded: schedule_input_030_log.json\n",
      "  âœ“ Loaded: schedule_input_031_log.json\n",
      "  âœ“ Loaded: schedule_input_032_log.json\n",
      "  âœ“ Loaded: schedule_input_033_log.json\n",
      "  âœ“ Loaded: schedule_input_034_log.json\n",
      "  âœ“ Loaded: schedule_input_035_log.json\n",
      "  âœ“ Loaded: schedule_input_036_log.json\n",
      "  âœ“ Loaded: schedule_input_037_log.json\n",
      "  âœ“ Loaded: schedule_input_038_log.json\n",
      "  âœ“ Loaded: schedule_input_039_log.json\n",
      "  âœ“ Loaded: schedule_input_040_log.json\n",
      "  âœ“ Loaded: schedule_input_041_log.json\n",
      "  âœ“ Loaded: schedule_input_042_log.json\n",
      "  âœ“ Loaded: schedule_input_043_log.json\n",
      "  âœ“ Loaded: schedule_input_044_log.json\n",
      "  âœ“ Loaded: schedule_input_045_log.json\n",
      "  âœ“ Loaded: schedule_input_046_log.json\n",
      "  âœ“ Loaded: schedule_input_047_log.json\n",
      "  âœ“ Loaded: schedule_input_048_log.json\n",
      "  âœ“ Loaded: schedule_input_049_log.json\n",
      "  âœ“ Loaded: schedule_input_050_log.json\n",
      "  âœ“ Loaded: schedule_input_051_log.json\n",
      "  âœ“ Loaded: schedule_input_052_log.json\n",
      "  âœ“ Loaded: schedule_input_053_log.json\n",
      "  âœ“ Loaded: schedule_input_054_log.json\n",
      "  âœ“ Loaded: schedule_input_055_log.json\n",
      "  âœ“ Loaded: schedule_input_056_log.json\n",
      "  âœ“ Loaded: schedule_input_057_log.json\n",
      "  âœ“ Loaded: schedule_input_058_log.json\n",
      "  âœ“ Loaded: schedule_input_059_log.json\n",
      "  âœ“ Loaded: schedule_input_060_log.json\n",
      "  âœ“ Loaded: schedule_input_061_log.json\n",
      "  âœ“ Loaded: schedule_input_062_log.json\n",
      "  âœ“ Loaded: schedule_input_063_log.json\n",
      "  âœ“ Loaded: schedule_input_064_log.json\n",
      "  âœ“ Loaded: schedule_input_065_log.json\n",
      "  âœ“ Loaded: schedule_input_066_log.json\n",
      "  âœ“ Loaded: schedule_input_067_log.json\n",
      "  âœ“ Loaded: schedule_input_068_log.json\n",
      "  âœ“ Loaded: schedule_input_069_log.json\n",
      "  âœ“ Loaded: schedule_input_070_log.json\n",
      "  âœ“ Loaded: schedule_input_071_log.json\n",
      "  âœ“ Loaded: schedule_input_072_log.json\n",
      "  âœ“ Loaded: schedule_input_073_log.json\n",
      "  âœ“ Loaded: schedule_input_074_log.json\n",
      "  âœ“ Loaded: schedule_input_075_log.json\n",
      "  âœ“ Loaded: schedule_input_076_log.json\n",
      "  âœ“ Loaded: schedule_input_077_log.json\n",
      "  âœ“ Loaded: schedule_input_078_log.json\n",
      "  âœ“ Loaded: schedule_input_079_log.json\n",
      "  âœ“ Loaded: schedule_input_080_log.json\n",
      "  âœ“ Loaded: schedule_input_081_log.json\n",
      "  âœ“ Loaded: schedule_input_082_log.json\n",
      "  âœ“ Loaded: schedule_input_083_log.json\n",
      "  âœ“ Loaded: schedule_input_084_log.json\n",
      "  âœ“ Loaded: schedule_input_085_log.json\n",
      "  âœ“ Loaded: schedule_input_086_log.json\n",
      "  âœ“ Loaded: schedule_input_087_log.json\n",
      "  âœ“ Loaded: schedule_input_088_log.json\n",
      "  âœ“ Loaded: schedule_input_089_log.json\n",
      "  âœ“ Loaded: schedule_input_090_log.json\n",
      "  âœ“ Loaded: schedule_input_091_log.json\n",
      "  âœ“ Loaded: schedule_input_092_log.json\n",
      "  âœ“ Loaded: schedule_input_093_log.json\n",
      "  âœ“ Loaded: schedule_input_094_log.json\n",
      "  âœ“ Loaded: schedule_input_095_log.json\n",
      "  âœ“ Loaded: schedule_input_096_log.json\n",
      "  âœ“ Loaded: schedule_input_097_log.json\n",
      "  âœ“ Loaded: schedule_input_098_log.json\n",
      "  âœ“ Loaded: schedule_input_099_log.json\n",
      "  âœ“ Loaded: schedule_input_100_log.json\n",
      "  âš  Missing: schedule_input_101_log.json\n",
      "  âš  Missing: schedule_input_102_log.json\n",
      "  âš  Missing: schedule_input_103_log.json\n",
      "  âš  Missing: schedule_input_104_log.json\n",
      "  âš  Missing: schedule_input_105_log.json\n",
      "  âš  Missing: schedule_input_106_log.json\n",
      "  âš  Missing: schedule_input_107_log.json\n",
      "  âš  Missing: schedule_input_108_log.json\n",
      "  âš  Missing: schedule_input_109_log.json\n",
      "  âš  Missing: schedule_input_110_log.json\n",
      "  âš  Missing: schedule_input_111_log.json\n",
      "  âš  Missing: schedule_input_112_log.json\n",
      "  âš  Missing: schedule_input_113_log.json\n",
      "  âš  Missing: schedule_input_114_log.json\n",
      "  âš  Missing: schedule_input_115_log.json\n",
      "  âš  Missing: schedule_input_116_log.json\n",
      "  âš  Missing: schedule_input_117_log.json\n",
      "  âš  Missing: schedule_input_118_log.json\n",
      "  âš  Missing: schedule_input_119_log.json\n",
      "  âš  Missing: schedule_input_120_log.json\n",
      "  âš  Missing: schedule_input_121_log.json\n",
      "  âš  Missing: schedule_input_122_log.json\n",
      "  âš  Missing: schedule_input_123_log.json\n",
      "  âš  Missing: schedule_input_124_log.json\n",
      "  âš  Missing: schedule_input_125_log.json\n",
      "  âš  Missing: schedule_input_126_log.json\n",
      "  âš  Missing: schedule_input_127_log.json\n",
      "  âš  Missing: schedule_input_128_log.json\n",
      "  âš  Missing: schedule_input_129_log.json\n",
      "  âš  Missing: schedule_input_130_log.json\n",
      "  âš  Missing: schedule_input_131_log.json\n",
      "  âš  Missing: schedule_input_132_log.json\n",
      "  âš  Missing: schedule_input_133_log.json\n",
      "  âš  Missing: schedule_input_134_log.json\n",
      "  âš  Missing: schedule_input_135_log.json\n",
      "  âš  Missing: schedule_input_136_log.json\n",
      "  âš  Missing: schedule_input_137_log.json\n",
      "  âš  Missing: schedule_input_138_log.json\n",
      "  âš  Missing: schedule_input_139_log.json\n",
      "  âš  Missing: schedule_input_140_log.json\n",
      "  âš  Missing: schedule_input_141_log.json\n",
      "  âš  Missing: schedule_input_142_log.json\n",
      "  âš  Missing: schedule_input_143_log.json\n",
      "  âš  Missing: schedule_input_144_log.json\n",
      "  âš  Missing: schedule_input_145_log.json\n",
      "  âš  Missing: schedule_input_146_log.json\n",
      "  âš  Missing: schedule_input_147_log.json\n",
      "  âš  Missing: schedule_input_148_log.json\n",
      "  âš  Missing: schedule_input_149_log.json\n",
      "  âš  Missing: schedule_input_150_log.json\n",
      "\n",
      "âœ“ Loaded 100 logs\n",
      "\n",
      "ðŸ“ˆ Interpolating...\n",
      "âœ“ Interpolated 100 runs\n",
      "\n",
      "ðŸ“Š Computing statistics...\n",
      "\n",
      "ðŸŽ¨ Generating plots...\n",
      "ðŸ“Š Saved: /Users/kaylali/Documents/Documents/Stanford/MSE311/Project/University_Course_Scheduler/Results/plots/aggregate_combined.png\n",
      "ðŸ“Š Saved: /Users/kaylali/Documents/Documents/Stanford/MSE311/Project/University_Course_Scheduler/Results/plots/all_runs_overlay.png\n",
      "ðŸ“„ Saved: /Users/kaylali/Documents/Documents/Stanford/MSE311/Project/University_Course_Scheduler/Results/plots/run_summary.csv\n",
      "\n",
      "============================================================\n",
      "SUMMARY STATISTICS\n",
      "============================================================\n",
      "Total runs: 100\n",
      "\n",
      "Initial Score:   Mean=68355.6, Std=13440.1\n",
      "Final Reduction: Mean=92.55%, Std=5.70%\n",
      "Solve Time:      Mean=1868.6s, Std=303.6s\n",
      "============================================================\n",
      "\n",
      "âœ… Done! Results saved to: /Users/kaylali/Documents/Documents/Stanford/MSE311/Project/University_Course_Scheduler/Results/plots\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "CURRENT_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.dirname(CURRENT_DIR)\n",
    "\n",
    "TRACKING_DIR = os.path.join(PROJECT_ROOT, \"Results\", \"tracking_logs\")\n",
    "PLOTS_DIR = os.path.join(PROJECT_ROOT, \"Results\", \"plots\")\n",
    "\n",
    "FILE_START = 1\n",
    "FILE_END = 150\n",
    "\n",
    "\n",
    "def load_tracking_logs(tracking_dir, file_start, file_end):\n",
    "    all_data = []\n",
    "    for i in range(file_start, file_end + 1):\n",
    "        filename = f\"schedule_input_{i:03d}_log.json\"\n",
    "        filepath = os.path.join(tracking_dir, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                data['file_id'] = i\n",
    "                all_data.append(data)\n",
    "                print(f\"  âœ“ Loaded: {filename}\")\n",
    "        else:\n",
    "            print(f\"  âš  Missing: {filename}\")\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def interpolate_to_common_times(all_data, num_points=100):\n",
    "    if not all_data:\n",
    "        return None, None, []\n",
    "    \n",
    "    max_times = []\n",
    "    for data in all_data:\n",
    "        if data['checkpoints']:\n",
    "            max_times.append(max(cp['time'] for cp in data['checkpoints']))\n",
    "    \n",
    "    if not max_times:\n",
    "        return None, None, []\n",
    "    \n",
    "    target_max_time = np.median(max_times)\n",
    "    common_times = np.linspace(0, target_max_time, num_points)\n",
    "    \n",
    "    interpolated_values = []\n",
    "    valid_runs = []\n",
    "    \n",
    "    for data in all_data:\n",
    "        checkpoints = data['checkpoints']\n",
    "        if len(checkpoints) < 2:\n",
    "            continue\n",
    "        \n",
    "        times = np.array([cp['time'] for cp in checkpoints])\n",
    "        pct_reduced = np.array([cp['pct_reduced'] for cp in checkpoints])\n",
    "        \n",
    "        sort_idx = np.argsort(times)\n",
    "        times = times[sort_idx]\n",
    "        pct_reduced = pct_reduced[sort_idx]\n",
    "        \n",
    "        unique_times, unique_idx = np.unique(times, return_index=True)\n",
    "        times = unique_times\n",
    "        pct_reduced = pct_reduced[unique_idx]\n",
    "        \n",
    "        if len(times) < 2:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            f = interpolate.interp1d(\n",
    "                times, pct_reduced,\n",
    "                kind='linear',\n",
    "                bounds_error=False,\n",
    "                fill_value=(pct_reduced[0], pct_reduced[-1])\n",
    "            )\n",
    "            interp_values = f(common_times)\n",
    "            interpolated_values.append(interp_values)\n",
    "            valid_runs.append(data['file_id'])\n",
    "        except Exception as e:\n",
    "            print(f\"  âš  Error interpolating file {data['file_id']}: {e}\")\n",
    "    \n",
    "    if not interpolated_values:\n",
    "        return None, None, []\n",
    "    \n",
    "    return common_times, np.array(interpolated_values), valid_runs\n",
    "\n",
    "\n",
    "def compute_statistics(interpolated_values):\n",
    "    mean = np.mean(interpolated_values, axis=0)\n",
    "    std = np.std(interpolated_values, axis=0)\n",
    "    n = interpolated_values.shape[0]\n",
    "    ci = 1.96 * std / np.sqrt(n)\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'ci_lower': mean - ci,\n",
    "        'ci_upper': mean + ci,\n",
    "        'median': np.median(interpolated_values, axis=0),\n",
    "        'q25': np.percentile(interpolated_values, 25, axis=0),\n",
    "        'q75': np.percentile(interpolated_values, 75, axis=0),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_combined(common_times, stats, num_runs, plots_dir):\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    ax.fill_between(common_times, stats['q25'], stats['q75'],\n",
    "                    alpha=0.25, color='#9b59b6', label='IQR (25th-75th)')\n",
    "    \n",
    "    ax.fill_between(common_times, stats['ci_lower'], stats['ci_upper'], \n",
    "                    alpha=0.4, color='#3498db', label='95% CI')\n",
    "    \n",
    "    ax.plot(common_times, stats['median'], linewidth=2, color='#27ae60',\n",
    "            linestyle='--', label='Median')\n",
    "    \n",
    "    ax.plot(common_times, stats['mean'], linewidth=3, color='#2c3e50',\n",
    "            label=f'Mean (n={num_runs})')\n",
    "    \n",
    "    ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "    ax.set_ylabel('Reduction in Penalty (%)', fontsize=12)\n",
    "    ax.set_title(f'Optimization Progress: Aggregate Results (Files {FILE_START:03d}-{FILE_END:03d})\\n'\n",
    "                 f'n={num_runs} runs', fontsize=14)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    final_mean = stats['mean'][-1]\n",
    "    final_ci = (stats['ci_upper'][-1] - stats['ci_lower'][-1]) / 2\n",
    "    ax.annotate(f'Final: {final_mean:.1f}% Â± {final_ci:.1f}%',\n",
    "                xy=(common_times[-1], final_mean),\n",
    "                xytext=(-120, 20), textcoords='offset points', fontsize=11,\n",
    "                arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(plots_dir, 'aggregate_combined.png')\n",
    "    plt.savefig(plot_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ“Š Saved: {plot_path}\")\n",
    "\n",
    "\n",
    "def plot_all_runs(all_data, common_times, stats, plots_dir):\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    for data in all_data:\n",
    "        checkpoints = data['checkpoints']\n",
    "        if len(checkpoints) < 2:\n",
    "            continue\n",
    "        times = [cp['time'] for cp in checkpoints]\n",
    "        pct_reduced = [cp['pct_reduced'] for cp in checkpoints]\n",
    "        ax.plot(times, pct_reduced, alpha=0.3, linewidth=1, color='#3498db')\n",
    "    \n",
    "    ax.plot(common_times, stats['mean'], linewidth=3, color='#e74c3c',\n",
    "            label=f'Mean (n={len(all_data)})')\n",
    "    \n",
    "    ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "    ax.set_ylabel('Reduction in Penalty (%)', fontsize=12)\n",
    "    ax.set_title(f'All Optimization Runs (Files {FILE_START:03d}-{FILE_END:03d})', fontsize=14)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(plots_dir, 'all_runs_overlay.png')\n",
    "    plt.savefig(plot_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ“Š Saved: {plot_path}\")\n",
    "\n",
    "\n",
    "def generate_summary_table(all_data, plots_dir):\n",
    "    rows = []\n",
    "    for data in all_data:\n",
    "        checkpoints = data['checkpoints']\n",
    "        if not checkpoints:\n",
    "            continue\n",
    "        initial_score = data.get('initial_score', checkpoints[0]['objective'])\n",
    "        final_checkpoint = checkpoints[-1]\n",
    "        rows.append({\n",
    "            'file_id': data['file_id'],\n",
    "            'filename': data['filename'],\n",
    "            'initial_score': initial_score,\n",
    "            'final_objective': final_checkpoint['objective'],\n",
    "            'final_pct_reduced': final_checkpoint['pct_reduced'],\n",
    "            'final_time': final_checkpoint['time'],\n",
    "            'num_checkpoints': len(checkpoints)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    csv_path = os.path.join(plots_dir, 'run_summary.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"ðŸ“„ Saved: {csv_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total runs: {len(df)}\")\n",
    "    print(f\"\\nInitial Score:   Mean={df['initial_score'].mean():.1f}, Std={df['initial_score'].std():.1f}\")\n",
    "    print(f\"Final Reduction: Mean={df['final_pct_reduced'].mean():.2f}%, Std={df['final_pct_reduced'].std():.2f}%\")\n",
    "    print(f\"Solve Time:      Mean={df['final_time'].mean():.1f}s, Std={df['final_time'].std():.1f}s\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(f\"AGGREGATE RESULTS: Files {FILE_START:03d}-{FILE_END:03d}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Current Dir:  {CURRENT_DIR}\")\n",
    "    print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "    print(f\"Tracking Dir: {TRACKING_DIR}\")\n",
    "    print(f\"Plots Dir:    {PLOTS_DIR}\")\n",
    "    \n",
    "    if not os.path.exists(TRACKING_DIR):\n",
    "        print(f\"\\nâŒ Tracking directory not found: {TRACKING_DIR}\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nðŸ“‚ Loading tracking logs...\")\n",
    "    all_data = load_tracking_logs(TRACKING_DIR, FILE_START, FILE_END)\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"\\nâŒ No tracking logs found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nâœ“ Loaded {len(all_data)} logs\")\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Interpolating...\")\n",
    "    common_times, interpolated_values, valid_runs = interpolate_to_common_times(all_data)\n",
    "    \n",
    "    if common_times is None:\n",
    "        print(\"\\nâŒ Failed to interpolate!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"âœ“ Interpolated {len(valid_runs)} runs\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š Computing statistics...\")\n",
    "    stats = compute_statistics(interpolated_values)\n",
    "    \n",
    "    print(\"\\nðŸŽ¨ Generating plots...\")\n",
    "    plot_combined(common_times, stats, len(valid_runs), PLOTS_DIR)\n",
    "    plot_all_runs(all_data, common_times, stats, PLOTS_DIR)\n",
    "    generate_summary_table(all_data, PLOTS_DIR)\n",
    "    \n",
    "    print(f\"\\nâœ… Done! Results saved to: {PLOTS_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
